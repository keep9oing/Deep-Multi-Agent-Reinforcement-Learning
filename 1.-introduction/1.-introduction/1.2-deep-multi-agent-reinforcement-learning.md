# 1.2 Deep Multi-Agent Reinforcement-Learning

최근 몇년간 DRL의 진보는 정적인 task의 single agent 환경에서 이루어 졌습다. 반면에 현실의 많은 문제들은 multi-agent problem으로 귀결됩니다.. 대부분의 상황에서 분산된 agent들은 전체적인 혹은 개인의 보상을 최대화하기 위해 다른 agent를 신경쓰면서 각각 불완전한 관찰을 통한 독립적인 결정을 내려야합니다. 심지어 사람까지도 포함해 신경써야할 수도 있습니.

Multi-Agent RL은 그러한 문제를 해결할 수 있는 하나의 프레임워크입니다. MARL은 알고리즘이나 학습 룰을 발전시키고 분석하는 걸 염두에 둡니다. 

이러한 환경을 넘어서, MARL은 다음과 같은 트렌드 때문에 중요한 역할을 할 것입니다.

첫째로, 머신러닝을 적용시킨 분야가 오고있습니다. 이는 AI System이 다른 환경에 있는 learning system을 염두에 둬야 할 것임을 의미합니다. 그렇지 않아 생긴 역효과 사례들이 있습니다. 

다음으로, 또 다른 이유는 MARL은 인간과 같은 시스템을 발전시키는데 있어 징검돌 역할을 할 수 있기 때문입니다. 사회적으로 많은 교류가 필요 할 수록, 지능이 높아야합니다. 남의 생각을 말하고, 남의 상태를 말하기 위해선 더 추상적이어야 하기 때문입니다.

여기선 그러한 Multi-Agent 상황에 대해 적용할 수 있는 새로운 알고리즘을 개발했는데, 여기선 collaboration, communication reciprocity에 집중해 설명을 합니다.

Collaboration은 많은 수의 agent가 동시에 배우면서 배우는 상황을 계속 nonstationary하게 계속변하는 점에서 문제가 발생합니다. 이는 전에 말했던, multi-agent credit assignment를 발생시킵니다. 이는 많은 agent가 계속 행동하므로, 어떤 행동때문에 좋은 보상 혹은 나쁜 보상을 얻었는지 할당하기가 어렵기 때문입니다. 환경내의 다른 agent들은 agent에게 reward를 할당하는데 교란요인이 됩니다.만약 한 agent가 최선의 선택을 하였다고 해도, 다른 agent의 행동에 달려있기 때문에, policy를 배우는데 어려움이 생기게 됩니다.

Communication은 communication protocol을 배우는 문제와 연관되어 있습니다. 많은 상황에서 agent은 decentralised actions을 하도록 설계되지만, 제한된 communication channel을 가집니다. 어떻게 상황에 대한 information을 교환할지는 어려운 문제를 푸는데 도움이 됩니다.

이렇듯 협력이 필요한 상황에서의 많은 문제는 해결되었지만, reciprocity는 다루기 어려운 문제입니다. 이러한 환경에서 agent는 다른 agent를 도왔을 때, 더많은 보상을 받습니다.  사람들은 이러한 능력이 자연스레 있지만, 이를 강화학습을 통해 학습시키는 것은 아직도 열린 문제입니다.

이러한 문제를 풀기 위해 여기서는 decentralised execution을 centralised training통해 배우도록 합니다. 이는, 실제 적용에는 모든 agent의 행동은 자신만의 observation을 통해 이뤄지지만, 학습할때는 centralised 된 방법으로, 추가적인 정보를 주는 방식입니다.

이는 Multi-Agent Reinforcement Learning전반에 걸쳐 사용되며, 심지어 비협력적관계의 학습에서도 적용이 됩니다!

