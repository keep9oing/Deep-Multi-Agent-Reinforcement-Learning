# 1.3  Overall Structure

이 후의 Section들은 Background에 대한 설명과 세 가지 파트로 나뉘는데, 여기서는 그 Section들에 대한 outline을 제시하고 간단한 overview를 진행합니다.

### Background

Background는 Chapter 2에서 다루게 됩니다. MARL에서 사용되는 formal한 setting과, 필요한 알고리즘, 개념적 지식을 설명합니다. 그 뒤의 내용들을 잘 이해하기 위해 Background의 내용은 확실히 소화하고 넘어가시길 바랍니다.

### Part 1 : Learning to Collaborate

Single agent의 테크닉을 그대로 MARL에 적용한 것을 Naive learning\(NL\) 이라고 합니다. 이 NL에서의 agent는 다른 상황을 모두 정적이라고 가정합니다. NL은 다른 agent에 대해 아무 가정없이 접근한다는 것이 키포인트입니다. NL은 신기하게도 생각보다 robust하고 좋은 성능을 보이는데, 그렇기에 다른 algorithm의 benchmark로 주로 사용됩니다.

이전에 언급했던 것 처럼 MARL에서의 큰 문제점은 credit assignment 문제인데, 한 agent가 최적의 행동을 하였어도 다른 agent때문에 전체의 reward가 줄어들면, 이러한 최적의 행동을 할 확률이 줄어들게 됩니다.이런 문제를 NL에서는 신경을 못쓰기 때문에 더욱 문제가 됩니다.

Chapter 3에서는 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)에 대해 배울 건데, COMA는 centralised critic을 사용해서 모든 agent간의 행동에 대한 value function을 사용하기 때문에 이러한 한계점을 극복할 수 있다. 아래에 설명한 difference reward에 착안해서 여기서는 value function을 counterfactual의 baseline을 계산하는데 사용했는데, 이 baseline은 다른 agent가 어떤 행동을 했을 때 어떤 일이 일어날지에 대한 평균에 대한 추정입니다. 이를 StarCraft micromanagement에 적용했는데, 좋은 baseline이 되었습니다.

COMA는 



### Difference reward

reward structure를 정할때, 가장 쉬운 접근은 모든 agent가 system이 받는대로 받는 것 입니다. 하지만 이는 너무 느린 수렴을 보이고, 그래서 여기서는 agent 개별 reward를 주는 것에 착안했습니다. agent들은 그들 각자의 reward를 최대화하는 것인데, 중요한 행동을 했을 때, 높은 reward를 주어 전체적인 시스템이 좋은 performance를 보이도록 해야합니다. 그래서 여기서 difference reward에 집중하였습니다.



### reference

* Kagan Tumer and Adrian Agogino. "Distributed Agent-Based Air Traffic Flow Management"\(2007\) 

