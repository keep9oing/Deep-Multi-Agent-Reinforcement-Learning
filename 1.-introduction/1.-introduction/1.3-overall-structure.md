# 1.3  Overall Structure

이 후의 Section들은 Background에 대한 설명과 세 가지 파트로 나뉘는데, 여기서는 그 Section들에 대한 outline을 제시하고 간단한 overview를 진행합니다.

### Background

Background는 Chapter 2에서 다루게 됩니다. MARL에서 사용되는 formal한 setting과, 필요한 알고리즘, 개념적 지식을 설명합니다. 그 뒤의 내용들을 잘 이해하기 위해 Background의 내용은 확실히 소화하고 넘어가시길 바랍니다.

## Part 1 : Learning to Collaborate

Single agent의 테크닉을 그대로 MARL에 적용한 것을 Naive learning\(NL\) 이라고 합니다. 이 NL에서의 agent는 다른 상황을 모두 정적이라고 가정합니다. NL은 다른 agent에 대해 아무 가정없이 접근한다는 것이 키포인트입니다. NL은 신기하게도 생각보다 robust하고 좋은 성능을 보이는데, 그렇기에 다른 algorithm의 benchmark로 주로 사용됩니다.

이전에 언급했던 것 처럼 MARL에서의 큰 문제점은 credit assignment 문제인데, 한 agent가 최적의 행동을 하였어도 다른 agent때문에 전체의 reward가 줄어들면, 이러한 최적의 행동을 할 확률이 줄어들게 됩니다.이런 문제를 NL에서는 신경을 못쓰기 때문에 더욱 문제가 됩니다.

### Chapter 3 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)

Chapter 3에서는 Counterfactual Multi-Agent Poilicy Gradidents\(COMA\)에 대해 배울 건데, COMA는 centralised critic을 사용해서 모든 agent간의 행동에 대한 value function을 사용하기 때문에 이러한 한계점을 극복할 수 있다. 아래에 설명한 difference reward\(1\)에 착안해서 여기서는 value function을 counterfactual의 baseline을 계산하는데 사용했는데, 이 baseline은 다른 agent가 어떤 행동을 했을 때 어떤 일이 일어날지에 대한 평균에 대한 추정입니다. 이를 StarCraft micromanagement에 적용했는데, 좋은 baseline이 되었습니다.

COMA는 agent들의 행동을 완전히 요소화하여 \(2\)joint value function을 배우게 됩니다. 다른말로, action이 여러 다른 agent들의 의한 joint action이 되도록 독립적으로 sampling 한다는 말입니다. 하지만 몇몇의 상황에서는 이러한 이러한 구성이 최적의 전략을 짜지 못할 수 있습니다. 

특히 한 agent의 최적의 행동이 다른 agent의 최적의 action을 뽑는데 독립적이지 않다면,  최적의 행동과 벗어난 행동을 하도록 만들 수 있습니다.

이러한 문제는 centralised controller에 의해 해결될 수 있는데, decentralised 된 execution에는 잘 작동하지 않을 수 있습니다.

### Chapter 4 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)

Chapter 4 에서 여기서는 Multi-Agent Common Knowledge Reinforcement Learning\(MACKRL\)를 소개합니다. MACKRL 에서는 group내에서 agent들이 서로 common knowledge를 가지게 되는데, 이는 fully decentralised 상황에서 joint action을 배우기 위함입니다. 그룹의 common knowledge라 함은 모든 agent가 알고 모든 agent가 모든 agent가 안다는 것을 아는 상태를 말합니다. 흥미로운건 MARL의 여러 환경에서 agent는 다른 agent들을 관찰하고, 그럼으로써 그들은 common knowledge 를 가지게 됩니다. 특히 MACKRL은 hierarchical controller에 의존하는데, 이 것은 그룹의 agent들에게 joint action 을 할당해주거나, 어떻게 agent들을 어떻게 분할해 협동해야하는지를 정해줍니다. 중요한 것은 각각의 세부 subgroup들은 조직화된 action을 고르는데 있어선 부족하지만 common knowledge 를 공유할 것입니다. 그러므로 MACKRL controller는 엄청난 양의 정보를 가지고 있는 개개인의 actor를 사용할 건지,   





### \(1\)Difference reward

reward structure를 정할때, 가장 쉬운 접근은 모든 agent가 system이 받는대로 받는 것 입니다. 하지만 이는 너무 느린 수렴을 보이고, 그래서 여기서는 agent 개별 reward를 주는 것에 착안했습니다. agent들은 그들 각자의 reward를 최대화하는 것인데, 중요한 행동을 했을 때, 높은 reward를 주어 전체적인 시스템이 좋은 performance를 보이도록 해야합니다. 그래서 여기서 difference reward에 집중하였습니다.

### \(2\)Joint

joint라는 단어가 자주 나오게 되는데 이는, 결합 확률 분포를 생각하면 쉽습니다.결합확률 분포는 확률 변수가 두가지 이상 결합되어있는 분포입니다. 다른 agent의 행동이 stochastic하므로 Joint value를 배우는 것이 정확한 표현입니다.

### reference

* Kagan Tumer and Adrian Agogino. "Distributed Agent-Based Air Traffic Flow Management"\(2007\) 

