# Table of contents

* [Deep Multi-Agent Reinforcement Learning](README.md)

## Abstract & Contents

* [Abstract](abstract-and-contents/deep-multi-agent-reinforcement-learning.md)
* [Contents](abstract-and-contents/untitled.md)

## 1. Introduction

* [1.1 The Industrial Revolution, Cognition, and Computers](1.-introduction/1.1-the-industrial-revolution-cognition-and-computers/README.md)
  * [Untitled](1.-introduction/1.1-the-industrial-revolution-cognition-and-computers/untitled.md)
* [1.2 Deep Multi-Agent Reinforcement-Learning](1.-introduction/1.2-deep-multi-agent-reinforcement-learning.md)
* [1.3  Overall Structure](1.-introduction/1.3-overall-structure.md)

## 2. Background

* [2.1 Reinforcement Learning](2.-background/2.1-reinforcement-learning.md)
* [2.2 Multi-Agent Settings](2.-background/2.2-multi-agent-settings.md)
* [2.3 Centralised vs Decentralised Control](2.-background/2.3-centralised-vs-decentralised-control.md)
* [2.4  Cooperative, Zero-sum, and General-Sum](2.-background/2.4-cooperative-zero-sum-and-general-sum.md)
* [2.5 Partial Observability](2.-background/2.5-partial-observability.md)
* [2.6 Centralised Training, Decentralised Execution](2.-background/2.6-centralised-training-decentralised-execution.md)
* [2.7 Value Functions](2.-background/2.7-value-functions.md)
* [2.8 Nash Equilibria](2.-background/2.8-nash-equilibria.md)
* [2.9 Deep Learning for MARL](2.-background/2.9-deep-learning-for-marl.md)
* [2.10 Q-Learning and DQN](2.-background/2.10-q-learning-and-dqn.md)
* [2.11 Reinforce and Actor-Critic](2.-background/2.11-reinforce-and-actor-critic.md)

## I Learning to Collaborate

* [3. Counterfactual Multi-Agent Policy Gradients](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/README.md)
  * [3.1 Introduction](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.1-introduction.md)
  * [3.2 Related Work](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.2-related-work.md)
  * [3.3 Multi-Agent StarCraft Micromanagement](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.3-multi-agent-starcraft-micromanagement.md)
  * [3.4 Methods](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/README.md)
    * [3.4.1 Independent Actor-Critic](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/3.4.1-independent-actor-critic.md)
    * [3.4.2 Counterfactual Multi-Agent Policy Gradients](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.4-methods/3.4.2-counterfactual-multi-agent-policy-gradients.md)
  * [3.5 Results](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.5-results.md)
  * [3.6 Conclusions & Future Work](i-learning-to-collaborate/3.-counterfactual-multi-agent-policy-gradients/3.6-conclusions-and-future-work.md)
* [4 Multi-Agent Common Knowledge Reinforcement Learning](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/README.md)
  * [4.1 Introduction](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.1-introduction.md)
  * [4.2 Related Work](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.2-related-work.md)
  * [4.3 Dec-POMDP and Features](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.3-dec-pomdp-and-features.md)
  * [4.4 Common Knowledge](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.4-common-knowledge.md)
  * [4.5 Multi-Agent Common Knowledge Reinforcement Learning](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.5-multi-agent-common-knowledge-reinforcement-learning.md)
  * [4.6 Pairwise MACKRL](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.6-pairwise-mackrl.md)
  * [4.7 Experiments and Results](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.7-experiments-and-results.md)
  * [4.8 Conclusion & Future Work](i-learning-to-collaborate/4-multi-agent-common-knowledge-reinforcement-learning/4.8-conclusion-and-future-work.md)
* [5 Stabilising Experience Replay](i-learning-to-collaborate/5-stabilising-experience-replay/README.md)
  * [5.1 Introduction](i-learning-to-collaborate/5-stabilising-experience-replay/5.1-introduction.md)
  * [5.2 Related Work](i-learning-to-collaborate/5-stabilising-experience-replay/5.2-related-work.md)
  * [5.3 Methods](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/README.md)
    * [5.3.1 Multi-Agent Importance Sampling](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/5.3.1-multi-agent-importance-sampling.md)
    * [5.3.2 Multi-Agent Fingerprints](i-learning-to-collaborate/5-stabilising-experience-replay/5.3-methods/5.3.2-multi-agent-fingerprints.md)
  * [5.4 Experiments](i-learning-to-collaborate/5-stabilising-experience-replay/5.4-experiments/README.md)
    * [5.4.1 Architecture](i-learning-to-collaborate/5-stabilising-experience-replay/5.4-experiments/5.4.1-architecture.md)
  * [5.5 Results](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/README.md)
    * [5.5.1 Importance Sampling](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.1-importance-sampling.md)
    * [5.5.2 Fingerprints](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.2-fingerprints.md)
    * [5.5.3 Informative Trajectories](i-learning-to-collaborate/5-stabilising-experience-replay/5.5-results/5.5.3-informative-trajectories.md)
  * [5.6 Conclusion & Future Work](i-learning-to-collaborate/5-stabilising-experience-replay/5.6-conclusion-and-future-work.md)

